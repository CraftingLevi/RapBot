{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RapLyricsGen.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/CraftingLevi/RapBot/blob/master/RapLyricsGen.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "dt5YtrR2N5VQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from math import floor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1Tp_oBIcbmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "d7c093d6-f698-4927-a1b7-a2109ed2b0de"
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.2)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (3.4.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.4)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Running setup.py bdist_wheel for PyDrive ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hG_Kkpjfc1rk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HCNQWCq-dLgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'https://drive.google.com/open?id=1GNlFjhpHylDYyS3VQauFzQxdLrwafeDu'\n",
        "download = drive.CreateFile({'id': '1GNlFjhpHylDYyS3VQauFzQxdLrwafeDu'})\n",
        "download.GetContentFile('Kanye West.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kbMUV_TXJVHZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This contains several functions that can be used to modify incoming data before passing through the model\n",
        "def load_collection():\n",
        "    file_location = \"Kanye West.json\"\n",
        "    data = json.load(open(file_location, 'r', encoding='ASCII'))\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_lyrics_artist(artist):\n",
        "    data = load_collection()[\"songs\"]\n",
        "    lyrics = [data[i]['lyrics'] for i in data]\n",
        "    return lyrics\n",
        "\n",
        "\n",
        "def combine_list(list):\n",
        "    out = ''\n",
        "    for i in list:\n",
        "        out += i\n",
        "    return out\n",
        "\n",
        "\n",
        "# INPUT\n",
        "# text to vectorize\n",
        "# OUTPUT\n",
        "# unique, contains a list of all characters that exist in a text\n",
        "# char2idx, a dict for character to id\n",
        "# idx2char, a dict for id to character\n",
        "\n",
        "def vectorize_text(text):\n",
        "    unique = sorted(set(text))\n",
        "    char2idx = {u: i for i, u in enumerate(unique)}\n",
        "    idx2char = {i: u for i, u in enumerate(unique)}\n",
        "    return unique, char2idx, idx2char\n",
        "\n",
        "\n",
        "# returns a dataset that has input_text and target_text\n",
        "def create_tensors(text, max_length=100, BUFFER_SIZE=10000, BATCH_SIZE=64):\n",
        "    l = len(text)\n",
        "    input_text = []\n",
        "    target_text = []\n",
        "    _, char2idx, idx2char = vectorize_text(text)\n",
        "    if floor(l / 100) < 10000:\n",
        "        print('Sequence count ({}) is smaller than recommended (10000)'.format(floor(l / 100)))\n",
        "    for f in range(0, l - max_length, max_length):\n",
        "        inps = text[f:f + max_length]\n",
        "        targ = text[f + 1:f + 1 + max_length]\n",
        "\n",
        "        input_text.append([char2idx[i] for i in inps])\n",
        "        target_text.append([char2idx[i] for i in targ])\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FErf7_z1JuPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2264
        },
        "outputId": "594f2f2d-586a-4a7d-a9dc-f23e659be2eb"
      },
      "cell_type": "code",
      "source": [
        "#This script will be able to generate lyrics based on any artist in the 'collection.json' file\n",
        "# Credits to the following sources:\n",
        "\n",
        "# Neural Text Generation: A Practical Guide by Ziang Xie\n",
        "# https://cs.stanford.edu/~zxie/textgen.pdf\n",
        "\n",
        "# A Link to the TensorFlow tutorial page for Text Generation\n",
        "# https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb\n",
        "\n",
        "# Conditional Language Models\n",
        "# https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b\n",
        "\n",
        "# Unsupervised Single Sentiment Neuron\n",
        "# https://blog.openai.com/unsupervised-sentiment-neuron/#sentimentneuron\n",
        "\n",
        "# mLSTM layer outperforms normal LSTM layers\n",
        "# https://arxiv.org/pdf/1609.07959.pdf\n",
        "\n",
        "import time\n",
        "import os\n",
        "\n",
        "max_length = 100\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "artist = \"Kanye West\"\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "text = combine_list(get_lyrics_artist(artist=artist))\n",
        "unique, char2idx, idx2char = vectorize_text(text=text)\n",
        "dataset = create_tensors(text=text, max_length=max_length, BUFFER_SIZE=BUFFER_SIZE,\n",
        "                         BATCH_SIZE=BATCH_SIZE)\n",
        "\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.units = units\n",
        "        self.batch_sz = batch_size\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        if tf.test.is_gpu_available():\n",
        "            self.gru = tf.keras.layers.CuDNNGRU(self.units,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True,\n",
        "                                                recurrent_initializer='glorot_uniform')\n",
        "        else:\n",
        "            self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                           return_sequences=True,\n",
        "                                           return_state=True,\n",
        "                                           recurrent_activation='sigmoid',\n",
        "                                           recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # output shape == (batch_size, max_length, hidden_size)\n",
        "        # states shape == (batch_size, hidden_size)\n",
        "\n",
        "        # states variable to preserve the state of the model\n",
        "        # this will be used to pass at every step to the model while training\n",
        "        output, states = self.gru(x, initial_state=hidden)\n",
        "\n",
        "        # reshaping the output so that we can pass it to the Dense layer\n",
        "        # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # The dense layer will output predictions for every time_steps(max_length)\n",
        "        # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, states\n",
        "\n",
        "\n",
        "model = Model(len(unique), embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
        "def loss_function(real, preds):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 model=model)\n",
        "\n",
        "# Training step\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    hidden = model.reset_states()\n",
        "\n",
        "    for (batch, (inp, target)) in enumerate(dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feeding the hidden state back into the model\n",
        "            # This is the interesting step\n",
        "            predictions, hidden = model(inp, hidden)\n",
        "\n",
        "            # reshaping the target because that's how the\n",
        "            # loss function expects it\n",
        "            target = tf.reshape(target, (-1,))\n",
        "            loss = loss_function(target, predictions)\n",
        "\n",
        "        grads = tape.gradient(loss, model.variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         loss))\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.9747\n",
            "Epoch 1 Batch 100 Loss 2.3138\n",
            "Epoch 1 Batch 200 Loss 1.9688\n",
            "Epoch 1 Loss 2.0022\n",
            "Time taken for 1 epoch 30.18234920501709 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.9261\n",
            "Epoch 2 Batch 100 Loss 1.7921\n",
            "Epoch 2 Batch 200 Loss 1.6460\n",
            "Epoch 2 Loss 1.6012\n",
            "Time taken for 1 epoch 30.130990028381348 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6045\n",
            "Epoch 3 Batch 100 Loss 1.5765\n",
            "Epoch 3 Batch 200 Loss 1.4591\n",
            "Epoch 3 Loss 1.4578\n",
            "Time taken for 1 epoch 30.192540884017944 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.4798\n",
            "Epoch 4 Batch 100 Loss 1.3552\n",
            "Epoch 4 Batch 200 Loss 1.4169\n",
            "Epoch 4 Loss 1.3279\n",
            "Time taken for 1 epoch 30.249562740325928 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.2245\n",
            "Epoch 5 Batch 100 Loss 1.2882\n",
            "Epoch 5 Batch 200 Loss 1.2345\n",
            "Epoch 5 Loss 1.2114\n",
            "Time taken for 1 epoch 30.254926204681396 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.2305\n",
            "Epoch 6 Batch 100 Loss 1.2144\n",
            "Epoch 6 Batch 200 Loss 1.2525\n",
            "Epoch 6 Loss 1.2255\n",
            "Time taken for 1 epoch 30.243353128433228 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0627\n",
            "Epoch 7 Batch 100 Loss 1.1145\n",
            "Epoch 7 Batch 200 Loss 1.1540\n",
            "Epoch 7 Loss 1.1050\n",
            "Time taken for 1 epoch 30.291618824005127 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.9882\n",
            "Epoch 8 Batch 100 Loss 1.1296\n",
            "Epoch 8 Batch 200 Loss 1.0871\n",
            "Epoch 8 Loss 1.0474\n",
            "Time taken for 1 epoch 30.353429317474365 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9845\n",
            "Epoch 9 Batch 100 Loss 0.9154\n",
            "Epoch 9 Batch 200 Loss 1.0389\n",
            "Epoch 9 Loss 1.0042\n",
            "Time taken for 1 epoch 30.299147605895996 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.8746\n",
            "Epoch 10 Batch 100 Loss 0.9068\n",
            "Epoch 10 Batch 200 Loss 0.9677\n",
            "Epoch 10 Loss 0.8724\n",
            "Time taken for 1 epoch 30.53348422050476 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.8843\n",
            "Epoch 11 Batch 100 Loss 0.8947\n",
            "Epoch 11 Batch 200 Loss 0.8833\n",
            "Epoch 11 Loss 1.0019\n",
            "Time taken for 1 epoch 30.29313588142395 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.8192\n",
            "Epoch 12 Batch 100 Loss 0.8575\n",
            "Epoch 12 Batch 200 Loss 0.8847\n",
            "Epoch 12 Loss 0.9006\n",
            "Time taken for 1 epoch 30.255088806152344 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.7705\n",
            "Epoch 13 Batch 100 Loss 0.7909\n",
            "Epoch 13 Batch 200 Loss 0.9080\n",
            "Epoch 13 Loss 0.7751\n",
            "Time taken for 1 epoch 30.113060474395752 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.7066\n",
            "Epoch 14 Batch 100 Loss 0.7981\n",
            "Epoch 14 Batch 200 Loss 0.8064\n",
            "Epoch 14 Loss 0.7866\n",
            "Time taken for 1 epoch 30.28246235847473 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.7083\n",
            "Epoch 15 Batch 100 Loss 0.8222\n",
            "Epoch 15 Batch 200 Loss 0.9181\n",
            "Epoch 15 Loss 0.8073\n",
            "Time taken for 1 epoch 30.368500471115112 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.7023\n",
            "Epoch 16 Batch 100 Loss 0.8109\n",
            "Epoch 16 Batch 200 Loss 0.8195\n",
            "Epoch 16 Loss 0.7399\n",
            "Time taken for 1 epoch 30.344520092010498 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.6604\n",
            "Epoch 17 Batch 100 Loss 0.6899\n",
            "Epoch 17 Batch 200 Loss 0.8045\n",
            "Epoch 17 Loss 0.7915\n",
            "Time taken for 1 epoch 30.33991765975952 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.6722\n",
            "Epoch 18 Batch 100 Loss 0.7544\n",
            "Epoch 18 Batch 200 Loss 0.7243\n",
            "Epoch 18 Loss 0.7523\n",
            "Time taken for 1 epoch 30.27036714553833 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.6165\n",
            "Epoch 19 Batch 100 Loss 0.7524\n",
            "Epoch 19 Batch 200 Loss 0.7900\n",
            "Epoch 19 Loss 0.7335\n",
            "Time taken for 1 epoch 30.314411401748657 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.6365\n",
            "Epoch 20 Batch 100 Loss 0.7026\n",
            "Epoch 20 Batch 200 Loss 0.7387\n",
            "Epoch 20 Loss 0.7174\n",
            "Time taken for 1 epoch 30.610267400741577 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f10c070e588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "j-CVRrIQLDBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "8472148d-25ea-4744-bf5c-4d5cd7226d5e"
      },
      "cell_type": "code",
      "source": [
        "# Evaluation step(generating text using the model learned)\n",
        "\n",
        "# number of characters to generate\n",
        "num_generate = 1000\n",
        "\n",
        "# You can change the start string to experiment\n",
        "start_string = 'S'\n",
        "# converting our start string to numbers(vectorizing!) \n",
        "input_eval = [char2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# empty string to store our results\n",
        "text_generated = ''\n",
        "\n",
        "# low temperatures results in more predictable text.\n",
        "# higher temperatures results in more surprising text\n",
        "# experiment to find the best setting\n",
        "temperature = 0.9\n",
        "\n",
        "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
        "hidden = [tf.zeros((1, units))]\n",
        "for i in range(num_generate):\n",
        "    predictions, hidden = model(input_eval, hidden)\n",
        "\n",
        "    # using a multinomial distribution to predict the word returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n",
        "    \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated += idx2char[predicted_id]\n",
        "\n",
        "print (start_string + text_generated)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "So they are wrote that profe to the people\n",
            "\n",
            "I am changed the bottomion. Written by Gooche, but she better well bad, they! Now, better love the day… Here  \n",
            "Produced by Kanye West for Very\n",
            "Got the sky baby going on that Uginter Seaversta ass got a children, wow... us here for the fur!\n",
            "\n",
            "\n",
            "Peniur love end, let them to go\n",
            "'Round and 'round they go\n",
            "I'd rather be strapped your bed and your friends get with me and my friends\n",
            "My friends, my friends, my friends, my friends?\n",
            "\n",
            "\n",
            "Mali, You’re CROUToby' Wiz any of us all some fly shadles\n",
            "But if I think there is born her\n",
            "And brick ont of me, that he crash\n",
            "Man, this devil wasn't no style be like Christmas of Khalics Lising, oh you feel something how impromp than the hitter at least nothing, it's always over\n",
            "Lovely-one night, add to slow is scubbosion\n",
            "A musician, for what I'm oblocing that this, dog\n",
            "Why are you so paranoid?\n",
            "Why are you gonna be, well\n",
            "Made me go\n",
            "Talk about some ass, that's what is it was into myself\n",
            "I mean as I forget the game into a bank \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}